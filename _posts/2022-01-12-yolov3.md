---
layout: post
title: "Yolo v2 + v3"
date: 2022-01-12 00:55:00 -500
tags: yolo
---



# Yolo v2 + v3



## Bounding Box Prediction

![yolov3 tensorflow 2](https://machinelearningspace.com/wp-content/uploads/2020/01/bbox_ok-2-868x1024.png)

![image-20220111092707234](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111092707234.png)

- The network predict 4 coordinates for each bounding box, $t_x$, $t_y$, $t_w$, $t_h$ .

  ![image-20220111094301108](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111094301108.png)

- $\hat t_* - t_*$ 값을 SSE로 사용
  $$
  b_x = \sigma(t_x) + c_x \\
  \sigma(t_x) = b_x - c_x \\
  t_x = \log(b_x - c_x)
  $$
  





## Class Prediction

- **softmax** 대신 **logistic classifiers**를 사용하고 loss는 **binary cross-entropy**를 사용.

  - **label**이 **중복**되는 경우(**Woman** and **Person** 과 같이 종속관계를 의미) 

    - ```
      Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data
      ```

    - ```
      softmax는 경우에 따라가 아니라 항상 각 박스가 정확히 하나의 정답을 갖는다. 그에 반해 multilabel approach는 경우에 따라 더 잘 모델링한다.
      ```

      

## Predictions Across Scales

-  yolov3는 3 가지 다른 크기에서 박스들을 예측한다. 우리의 시스템은 Feature Pyramid Network와 비슷한 컨셉을 사용하여 이러한 크기로부터 특징을 추출한다. 기본적인 extractor에서 우리는 몇가지 convolutional layers를 추가했다. 마지막 layer는 3-d tensor로 encdoing 된 boudning box, objectness, 그리고 class들을 예측한다. COCO 데이터셋에서 우리는 각 크기마다 3개의 박스를 예측했고 마지막 tensor는 (N x N x [3(box 수) * (4(x,y,w,h) + 1(confidence score) + 80(labels))]) 갖는다.

- 다음으로 우리는 두 개 layers들로부터 feature map을 가져오고 upsampling 2x 한다. 우리는 미리 layer에서 feature map을 가져오고 upsampled features와 concat([1] + [2] = [1, 2])한다. 이러한 방법은 우리에게 더 의미 있는 upsmapled된 semantic 정보와 미리 가져온 feature map에서 finer-grained 정보를 준다.그리고 우리는 몇가지 convolutional layers를 combined feature map을 처리하기 위해 추가하고 결론적으로 2배 사이즈 되는 크기에 비슷한 tensor들을 예측한다.

-  우리는 마지막 scale에 위와 같은 디자인을 한 번 더 수행한다. 따라서 3d scale 예측은 이전 모든 계산 뿐만 아니라 fine-grained feautre로 부터 이득이다.

- 우리는 여전히 boungding box priors을 결정하기 위해 k-means clustering을 사용하고 있다. 우리는 선택된 9개의 cluster를 정렬하고 임의로 9개의 cluster와 3개의 스케일을 선택한 다음 cluster를 scale에 균등하게 나눈다. On the COCO dataset the 9 clusters were: (10×13),(16×30),(33×23),(30×61),(62×45),(59×119),(116 × 90),(156 × 198),(373 × 326).

  ### Yolov2

  - **Feature Pyramid Network**

    - **Feature Pyramid Network**는 임의의 크기의 **single-scale** 이미지를 convolutional network에 입력하여 다양한 scale의 feature map을 출력하는 네트워크입니다. 논문에서는 **ResNet**을 사용합니다. **FPN는 완전 새롭게 설계된 모델이 아니라 기존 convolutional network에서 지정한 layer별로 feature map을 추출하여 수정하는 네트워크라고 이해하시면 됩니다.** FPN이 feature map을 추출하여 피라미드를 건축하는 과정은 **bottom-up pathway, top-down pathway, lateral connections**에 따라 진행됩니다. 
    - **Fine-Grained Features**
      - 

  - **Anchors**

    - YOLOv1 -> YOLOv2

    - **Convolutional With Anchor Boxes**

      ```
       YOLO predicts the coordinates of bounding boxes directly using fully connected layers on top of the convolutional feature extractor.Instead of predicting coordinates directly Faster R-CNN predicts bounding boxes using hand-picked priors [15]. Using only convolutional layers the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Since the prediction layer is convolutional, the RPN predicts these offsets at every location in a feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn.
       We remove the fully connected layers from YOLO and use anchor boxes to predict bounding boxes. First we eliminate one pooling layer to make the output of the network’s convolutional layers higher resolution. We also shrink the network to operate on 416 input images instead of 448×448. We do this because we want an odd number of locations in our feature map so there is a single center cell. Objects, especially large objects, tend to occupy the center of the image so it’s good to have a single location right at the center to predict these objects instead of four locations that are all nearby. YOLO’s convolutional layers downsample the image by a factor of 32 so by using an input image of 416 we get an output feature map of 13 × 13.
       When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. Following YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box and the class predictions predict the conditional probability of that class given that there is an object.
       Using anchor boxes we get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes our model predicts more than a thousand. Without anchor boxes our intermediate model gets 69.5 mAP with a recall of 81%. With anchor boxes our model gets 69.2 mAP with a recall of 88%. Even though the mAP decreases, the increase in recall means that our model has more room to improve.
      ```

      ```
       yolo는 fully connected layers를 사용해 직접 박스의 좌표를 예측한다. Faster R-CNN은 좌표를 직접 예측하는 것 대신 bounding box의 hand-picked priors(anchors w, h)를 예측한다. RPN(region proposal network)은 anchor box의 offsets과 confidences를 예측한다. prediction layer가 convolutional하기 때문에 RPN은 feature map에 모든 location의 offsets을 예측한다. coordinates를 예측하는 것보다 offsets을 예측하는 것은 문제를 간단하게 처리하고 network를 배우기 쉽게 한다.
       우리는 yolo에서 bounding boxes를 예측하기 위해 FC를 없애고 anchor boxes를 사용했다. 마지막 convolution layer을 높은 해상도로 만들기 위해 pooling layer를 하나 제거했다. 그리고 input layer를 448x448에서 416x416으로 수축했다. 이는 odd number location을 만들기 위해서였고 그래서 416으로 수정했을 때 single center cell을 갖는다. 특히 큰 object는 이미지 중앙에 위치할 텐데 이는 곧 4개보다는 한개에 있을 때가 더 좋은 결과일 것이다. YOLO convolutional layers은 32개 부분으로(32 x 13 = 416) downsample한다.
       When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. (anchor box를 움직였을 때 class prediction mechanism을 분리했다...?) YOLO처럼, objectness prediction은 IOU와 box를 예측하고 class prediction은 주어진 object에 conditional 확률을 예측한다.
       anchor box를 사용하면서 약간의 정확도는 감소했다. YOLOv1은 98개 box만 사용했지만 v2는 수천개 이상을 예측한다. anchor boxes없이는 69.5 mAP, recall 81%를 기록, anchor boxes를 적용하고 69.2 mAP, recall 88% 기록했다. recall의 증가는 개선 여지가 많다는 것을 의미한다.
      ```

      -  yolo는 fully connected layers를 사용해 직접 박스의 좌표를 예측한다. Faster R-CNN은 좌표를 직접 예측하는 것 대신 bounding box의 hand-picked priors(anchors w, h)를 예측한다. RPN(region proposal network)은 anchor box의 offsets과 confidences를 예측한다. prediction layer가 convolutional하기 때문에 RPN은 feature map에 모든 location의 offsets을 예측한다. coordinates를 예측하는 것보다 offsets을 예측하는 것은 문제를 간단하게 처리하고 network를 배우기 쉽게 한다.
      - 우리는 yolo에서 bounding boxes를 예측하기 위해 FC를 없애고 anchor boxes를 사용했다. 마지막 convolution layer을 높은 해상도로 만들기 위해 pooling layer를 하나 제거했다. 그리고 input layer를 448x448에서 416x416으로 수축했다. 이는 odd number location을 만들기 위해서였고 그래서 416으로 수정했을 때 single center cell을 갖는다. 특히 큰 object는 이미지 중앙에 위치할 텐데 이는 곧 4개보다는 한개에 있을 때가 더 좋은 결과일 것이다. YOLO convolutional layers은 32개 부분으로(32 x 13 = 416) downsample한다.
      -  When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. (anchor box를 움직였을 때 class prediction mechanism을 분리했다...?) YOLO처럼, objectness prediction은 IOU와 box를 예측하고 class prediction은 주어진 object에 conditional 확률을 예측한다.
      -  anchor box를 사용하면서 약간의 정확도는 감소했다. YOLOv1은 98개 box만 사용했지만 v2는 수천개 이상을 예측한다. anchor boxes없이는 69.5 mAP, recall 81%를 기록, anchor boxes를 적용하고 69.2 mAP, recall 88% 기록했다. recall의 증가는 개선 여지가 많다는 것을 의미한다.

  - **Dimension Clusters**

    ![image-20220111160050362](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111160050362.png)

    

    

    ```
     We encounter two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. The network can learn to adjust the boxes appropriately but if we pick better priors for the network to start with we can make it easier for the network to learn to predict good detections. 
     Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with
    Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use d(box, centroid) = 1 − IOU(box, centroid)
     We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k = 5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different
    than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.
     We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If
    we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn
    ```

    ```
     우리는 두 가지 issues를 마주했다. 하나는 상자 크기를 직접 선택해야 한다는 것이다. network는 적절하게 박스를 학습할 수 있지만 처음부터 잘 선택하면 더 쉽게 학습할 수 있다.
     직접 priors를 정하는 것보다 k-means clustering을 사용하면 자동으로 좋은 prior를 찾을 수 있다. 만약 유클리드 거리를 이용해 standard k-means를 사용한다면 더 큰 box일수록 더 높은 error가 나올 것 이다. 하지만 높은 IOU를 갖는 priors를 원하기에 size에 독립적이여야한다. 따라서 아래와 같은 수식을 사용한다.
     					d(box, centroid) = 1 − IOU(box, centroid)
     k-means을 다양한 k에 돌려 plot을 찍어봤을 때, k=5에서 좋은 tradeoff 관계를 갖는다. 이 cluster의 centroids는 직접 정한 priors보다 더 중요한 결과를 갖는다. 데이터에는 짧고 넓은 박스는 적었고 길고 얇은 박스가 많았다. (차 < 사람)
     Table 1, k=5일 때는 별 차이없는데 k=9일때 확연한 차이를 확인할 수 있다.
     
    ```

    - 우리는 두 가지 issues를 마주했다. 하나는 상자 크기를 직접 선택해야 한다는 것이다. network는 적절하게 박스를 학습할 수 있지만 처음부터 잘 선택하면 더 쉽게 학습할 수 있다.

    - 직접 priors를 정하는 것보다 k-means clustering을 사용하면 자동으로 좋은 prior를 찾을 수 있다. 만약 유클리드 거리를 이용해 standard k-means를 사용한다면 더 큰 box일수록 더 높은 error가 나올 것 이다. 하지만 높은 IOU를 갖는 priors를 원하기에 size에 독립적이여야한다. 따라서 아래와 같은 수식을 사용한다.
      $$
      d(box, centroid) = 1 − IOU(box, centroid)
      $$
      

    -  k-means을 다양한 k에 돌려 plot을 찍어봤을 때, k=5에서 좋은 tradeoff 관계를 갖는다. 이 cluster의 centroids는 직접 정한 priors보다 더 중요한 결과를 갖는다. 데이터에는 짧고 넓은 박스는 적었고 길고 얇은 박스가 많았다. (차 < 사람)
       Table 1, k=5일 때는 별 차이없는데 k=9일때 확연한 차이를 확인할 수 있다.

      ![image-20220111163135127](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111163135127.png)

    - **Direct location prediction**

      ```
      When using anchor boxes
      with YOLO we encounter a second issue: model instability,
      especially during early iterations. Most of the instability
      comes from predicting the (x, y) locations for the box. In
      region proposal networks the network predicts values tx and
      ty and the (x, y) center coordinates are calculated as:
      
      For example, a prediction of tx = 1 would shift the box
      to the right by the width of the anchor box, a prediction of
      tx = −1 would shift it to the left by the same amount.
      This formulation is unconstrained so any anchor box can
      end up at any point in the image, regardless of what location predicted the box. With random initialization the model
      takes a long time to stabilize to predicting sensible offsets.
      Instead of predicting offsets we follow the approach of
      YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall
      between 0 and 1. We use a logistic activation to constrain
      the network’s predictions to fall in this range.
      The network predicts 5 bounding boxes at each cell in
      the output feature map. The network predicts 5 coordinates
      for each bounding box, tx, ty, tw, th, and to. If the cell is
      offset from the top left corner of the image by (cx, cy) and
      the bounding box prior has width and height pw, ph, then
      the predictions correspond to:
      
      Since we constrain the location prediction the
      parametrization is easier to learn, making the network
      more stable. Using dimension clusters along with directly
      predicting the bounding box center location improves
      YOLO by almost 5% over the version with anchor boxes
      ```

      - 두번째 issue는 특히 초반 iterations에서 model instability이다. 대부분 instability는 box location $x$, $y$를 예측하는데 발생한다. region proposal networks에서는 $t_x$, $t_y$을 예측하고 $x, y$는 아래와 같다.
        $$
        x = (t_x * w_a) - w_a\\
        y = (t_y * h_a) - y_a
        $$

      - 예를들어 $t_x = 1$이라면 오른쪽 anchor box로 이동한다. $t_x = -1$이라면 왼쪽으로 이동한다.

      - 위와 같은 공식은 제약이 없다. 그래서 어떤 location에서 박스를 예측했는지 관계없이 anchor box는 이미지 어느 점이라도 위치할 수 있다. 초반에 random으로 초기화된 모델은 좋은 offsets을 찾기 위해 긴 시간을 안정화한다.

      - offsets을 예측하는 대신 각 grid cell에 연관된 location 좌표를 예측하는 방식으로 YOLO의 접근을 따라 갈 있다. 이러한 방법은 ground truth가 0과 1사이에 위치하도록 제한한다. 그래서 우리는 특정 범위에 속하게 하기위해 logistic activation을 사용한다.

      - network는 output feature map에서 각 cell에 5개 bounding boxes를 예측한다. 각 bounding box는 5개 좌표를 갖고 있다. $(t_x, t_y, t_w, t_h, t_o)$. 만약 각 cell이 offset from the top left corner of the image by $(c_x, c_y)$이고 bounding box prior has width and height $p_w, p_h$이라면 예측은 아래와 대응된다.
        $$
        b_x = \sigma(t_x) + c_x \\
        b_y = \sigma(t_y) + c_y \\
        b_w = p_w e^{t_w} \  \ \ \ \ \   \ \\
        b_h = p_h e^{t_h}\ \ \ \ \ \  \ \ \\
        Pr(object) * IOU(b, object) = \sigma(t_\sigma)
        $$

      - location prediction을 학습하기 쉽게 제약했기 때문에 network는 더 안정적이다. 위와 같은 방식을 사용했더니 5% 더 향상했다.

    - **Fine-Grained Features**

      ```
      This modified YOLO predicts
      detections on a 13 × 13 feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. Faster R-CNN and SSD both run their proposal networks at various feature maps in
      the network to get a range of resolutions. We take a different approach, simply adding a passthrough layer that brings
      features from an earlier layer at 26 × 26 resolution.
      The passthrough layer concatenates the higher resolution
      features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the 26 × 26 × 512 feature map into a 13 × 13 × 2048
      feature map, which can be concatenated with the original
      features. Our detector runs on top of this expanded feature map so that it has access to fine grained features. This gives a modest 1% performance increase.
      ```

      - 수정된 YOLO는 13x13 feature map을 예측한다(416 = 13 x 32). 큰 object에 충분하다면 작은 object들에게 finer grained features로부터 이득을 얻을 수 있다.  Faster R-CNN과 SSD(Single Shot Multiple box Detection)은 해상도 범위에서 다양한 feature maps에 proposal networks를 돌아간다? 다양한 접근을 했고 간단히 26x26 resolution layer를 features를 추가하면서 passthrough를 도입했다. 

      - passthrough layer는 높은 resolution features 와 low resolution feautres를 다른 채널에 쌓음으로써 concat한다. 이러한 방법은 26 x 26 x 512 feature map은 original features과 합쳐지면서 13 x 13 x 2048로 바꾼다. (?, 그림필요) detector는 fine grained features에 접근하기 위해 확장된 feature map에서 돌아간다. 이는 1% 향상을 제공한다.

        ```python
        class Bottleneck(nn.Module):
            # Standard bottleneck
            def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
                super().__init__()
                c_ = int(c2 * e)  # hidden channels
                self.cv1 = Conv(c1, c_, 1, 1)
                self.cv2 = Conv(c_, c2, 3, 1, g=g)
                self.add = shortcut and c1 == c2
        
            def forward(self, x):
                return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
        ```

        

    - **Multi-Scale Training**

      ```
      The original YOLO uses an input resolution of 448 × 448. With the addition of anchor boxes we changed the resolution to 416×416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model.
       Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.
       This regime forces the network to learn to predict well across a variety of input dimensions. This means the same network can predict detections at different resolutions. The network runs faster at smaller sizes so YOLOv2 offers an easy tradeoff between speed and accuracy.
       At low resolutions YOLOv2 operates as a cheap, fairly accurate detector. At 288 × 288 it runs at more than 90 FPS with mAP almost as good as Fast R-CNN. This makes it ideal for smaller GPUs, high framerate video, or multiple video streams.
       At high resolution YOLOv2 is a state-of-the-art detector with 78.6 mAP on VOC 2007 while still operating above real-time speeds. See Table 3 for a comparison of YOLOv2 with other frameworks on VOC 2007. Figure
      ```

      - original YOLO는 input 해상도를 448 x 448을 썼다. anchor box를 추가하면서 해상도는 416 x 416으로 바꿨다. 하지만 convolutional과 pooling layer만 사용했기 때문에 우리는 즉각적으로 resized를 할 수 있다. YOLOv2가 다양한 이미지 사이즈에서 실행되기를 원하므로 this?를 훈련시킨다.

      - input image size를 수정하는 대신 network를 매번 몇 iterations을 바꾼다. 매 10개 배치 마다 랜덤으로 10개 이미지 size를 정한다. 모델은 32개로 downsample하기 때문에 multiples of 32: {320, 352, ..., 608} pull한다. 따라서 가장 작은 option은 320 x 320 이고 가장 큰 것은 608 x 608이다. 훈련하면서 계속 size를 바꾼다.

      - 이러한 체제는 network를 다양한 해상도에서 학습하도록 한다. 그래서 같은 network로 다양한 해상도를 예측할 수 있다. 당연히 가장 작은 size에서 speed와 acc사이에 쉬운 tradeoff를 제공한다.

      - 낮은 해상도에서는 빠르고 공평히 높은 detector이다. 288에서 90fps보다 높고 Fast R-CNN만큼의 mAP를 제공한다. 이것은 작은 gpu, 높은 fps video, multiple video stream을 제공한다.

      - 높은 해상도에서, state-of-the-art detector는 VOC 2007에서 78.6 mAP real-time speed에서. table 3 비교표를 보자!

        ![image-20220111182531803](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111182531803.png)

## Feature Extractor

```
 We use a new network for performing feature extraction. Our new network is a hybrid approach between the network used in YOLOv2, Darknet-19, and that newfangled residual network stuff. Our network uses successive 3 × 3 and 1 × 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so we call it.... wait for it..... Darknet-53!
 This new network is much more powerful than Darknet19 but still more efficient than ResNet-101 or ResNet-152. Here are some ImageNet results:
 Each network is trained with identical settings and tested at 256×256, single crop accuracy. Run times are measured on a Titan X at 256 × 256. Thus Darknet-53 performs on par with state-of-the-art classifiers but with fewer floating point operations and more speed. Darknet-53 is better than ResNet-101 and 1.5× faster. Darknet-53 has similar performance to ResNet-152 and is 2× faster.
 Darknet-53 also achieves the highest measured floating point operations per second. This means the network structure better utilizes the GPU, making it more efficient to evaluate and thus faster. That’s mostly because ResNets have just way too many layers and aren’t very efficient.
```

- feature extraction을 하기위해 새로운 network를 사용한다. 새로운 network는 이전 모듈의 hybrid 접근이다.(...?) network는 연속적인 3 x 3 그리고 1 x 1 convolutional layer를 사용하고 몇가지 shortcut connection을 사용한다. 53개를 가져서 Darknet-53?

  ![image-20220111191809380](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111191809380.png)

- 새로운 network는 darknet19보다 매우 강력하고 resnet-101 또는 resnet-152보다 효율적이다. 

  ![image-20220111191823516](C:\Users\ha421\AppData\Roaming\Typora\typora-user-images\image-20220111191823516.png)

- 각 network는 동일한 설정으로 훈련되고 256 x 256 crop으로 정확도 테스트를 합니다....